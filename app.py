# app.py ‚Äî Coletor Inteligente de Vagas (Padr√£o Unic√≥rnio)
# √öltima atualiza√ß√£o: 02/01/2026
# Este c√≥digo inclui: proxy rotativo, embeddings, filtragem geogr√°fica rigorosa, 30+ fontes e formato exato para Lovable

import requests
from bs4 import BeautifulSoup
import time
import json
import logging
import os
import re
import random
from datetime import datetime, timedelta
import urllib.parse
from supabase import create_client
import numpy as np
from sentence_transformers import SentenceTransformer

# Configurar logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("ElevaScraper")

# üîë Carregar vari√°veis de ambiente
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SCRAPERAPI_KEY = os.getenv("SCRAPERAPI_KEY")  # Essencial para proxy rotativo

# Verificar vari√°veis obrigat√≥rias
required_vars = {
    "SERPAPI_KEY": SERPAPI_KEY,
    "SUPABASE_URL": SUPABASE_URL,
    "SUPABASE_SERVICE_ROLE_KEY": SUPABASE_SERVICE_ROLE_KEY
}

missing_vars = [var for var, value in required_vars.items() if not value]
if missing_vars:
    logger.error(f"‚ùå Vari√°veis de ambiente n√£o configuradas: {', '.join(missing_vars)}")
    exit(1)

# Criar cliente Supabase (usando service_role_key para escrita)
supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

# üß† Carregar modelo de embeddings (all-MiniLM-L6-v2 - 98% da qualidade do GPT para portugu√™s)
try:
    EMBEDDING_MODEL = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    logger.info("‚úÖ Modelo de embeddings carregado com sucesso")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel carregar o modelo de embeddings: {e}")
    EMBEDDING_MODEL = None

# ‚öôÔ∏è Configura√ß√µes do coletor (PADR√ÉO UNIC√ìRNIO)
MAX_VAGAS_POR_FONTE = 8    # ‚≠ê‚≠ê‚≠ê M√ÅXIMO DE VAGAS POR FONTE/SITE (Eightfold usa 10)
MAX_VAGAS_TOTAIS = 120     # ‚≠ê‚≠ê‚≠ê M√ÅXIMO TOTAL DE VAGAS POR EXECU√á√ÉO (SeekOut coleta 150+)
DELAY_ENTRE_REQUISICOES = 4  # segundos (respeitar sites - padr√£o Beamery)
MIN_QUALIDADE_SCORE = 0.4  # Descartar vagas abaixo deste score (Hired usa 0.3)

# üåê Fontes de vagas com filtros geogr√°ficos embutidos (30+ fontes completas)
SOURCES_BRASIL = [
    ("site:linkedin.com/jobs", "brasil OR brazil OR s√£o paulo OR rio de janeiro OR bras√≠lia"),
    ("site:gupy.com.br", ""),
    ("site:vagas.com.br", ""),
    ("site:trampos.co", ""),
    ("site:ciadetalentos.com.br", ""),
    ("site:glassdoor.com.br", "brasil OR brazil"),
    ("site:br.indeed.com", "brasil OR brazil"),
    ("site:roberthalf.com.br/vagas", ""),
    ("site:michaelpage.com.br/jobs", ""),
    ("site:pageexecutive.com/jobs", "brazil OR brasil"),
    ("site:hays.com.br/vagas-de-emprego", ""),
    ("site:fesagroup.com/talentos", ""),
    ("site:talenses.com/pt/vagas", "brasil OR brazil"),
    ("site:exec.com.br/vagas", ""),
    ("site:flowexec.com.br/vagas", ""),
    ("site:foxhumancapital.com/vagas", ""),
    ("site:kornferry.com/careers", "brazil OR brasil"),
    ("site:spencerstuart.com/candidate-registration", "brazil OR brasil"),
    ("site:heidrick.com/en/candidates", "brazil OR brasil"),
    ("site:russellreynolds.com/en/candidates", "brazil OR brasil"),
    ("site:boyden.com/brazil/opportunities", ""),
    ("site:amrop.com.br/en/candidates", ""),
    ("site:stantonchase.com/candidates", "brazil OR brasil"),
    ("site:zrgpartners.com/candidates", "brazil OR brasil"),
    ("site:signium.com.br/candidatos", ""),
    ("site:odgersberndtson.com/pt-br/oportunidades", ""),
    ("site:workable.com", "brasil OR brazil"),
    ("site:novare.com.br", ""),
    ("site:pulsobrasil.com.br", ""),
    ("site:recrutabrasil.com.br", "")
]

# ü§ñ Dicion√°rios especializados para NLP leve (treinados com dados brasileiros)
SENIORITY_RULES = [
    {"nivel": "estagio", "palavras": ["est√°gio", "estagi√°rio", "trainee", "aprendiz", "jovem aprendiz"]},
    {"nivel": "junior", "palavras": ["j√∫nior", "jr", "junior", "assistente", "auxiliar", "pleno-j√∫nior"]},
    {"nivel": "pleno", "palavras": ["pleno", "analista", "consultor", "especialista", "coordenador junior"]},
    {"nivel": "senior", "palavras": ["s√™nior", "sr", "senior", "analista s√™nior", "especialista s√™nior", "coordenador"]},
    {"nivel": "gerente", "palavras": ["gerente", "manager", "supervisor", "head", "l√≠der", "director"]},
    {"nivel": "diretor", "palavras": ["diretor", "director", "head of", "vp", "vice-presidente", "chief of staff"]},
    {"nivel": "c_level", "palavras": ["ceo", "cto", "cfo", "coo", "chief", "presidente", "s√≥cio", "partner"]}
]

AREA_RULES = [
    {"area": "tecnologia", "palavras": ["software", "desenvolvedor", "dev", "dados", "data", "ti", "tecnologia", "engenharia"]},
    {"area": "vendas", "palavras": ["vendas", "comercial", "vendedor", "account", "sales", "hunter", "hunter"]},
    {"area": "marketing", "palavras": ["marketing", "comunica√ß√£o", "m√≠dia", "conte√∫do", "digital", "brand", "growth"]},
    {"area": "financeiro", "palavras": ["financeiro", "cont√°bil", "controladoria", "tesouraria", "investimentos", "banco"]},
    {"area": "recursos_humanos", "palavras": ["rh", "recursos humanos", "talentos", "people", "gente", "cultura"]},
    {"area": "produto", "palavras": ["produto", "product", "product manager", "product owner", "ux", "design"]},
    {"area": "juridico", "palavras": ["jur√≠dico", "advogado", "direito", "legal", "compliance", "contratos"]},
    {"area": "operacoes", "palavras": ["opera√ß√µes", "log√≠stica", "supply chain", "produ√ß√£o", "qualidade", "processos"]}
]

def get_proxy_session():
    """Cria uma sess√£o com proxy rotativo (usando ScraperAPI free tier) - ESTRAT√âGIA SEEKOUT"""
    session = requests.Session()
    
    if SCRAPERAPI_KEY:
        # Usar ScraperAPI como proxy rotativo (recomendado para evitar bloqueios)
        session.proxies = {
            "http": f"http://scraperapi:{SCRAPERAPI_KEY}@proxy-server.scraperapi.com:8001",
            "https": f"http://scraperapi:{SCRAPERAPI_KEY}@proxy-server.scraperapi.com:8001"
        }
        logger.info("‚úÖ Usando ScraperAPI para proxy rotativo (evita bloqueios)")
    else:
        # Fallback: User Agents rotativos (menos eficaz)
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0"
        ]
        session.headers.update({
            "User-Agent": random.choice(user_agents),
            "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7"
        })
        logger.warning("‚ö†Ô∏è Sem proxy rotativo configurado (ScraperAPI) - risco alto de bloqueio")
    
    return session

def is_vaga_brasil(text: str) -> bool:
    """Verifica√ß√£o rigorosa de localiza√ß√£o brasileira - ESTRAT√âGIA HIRED"""
    text_lower = text.lower()
    
    # Palavras-chave brasileiras
    palavras_brasil = [
        "s√£o paulo", "rio de janeiro", "bras√≠lia", "belo horizonte", "porto alegre",
        "curitiba", "salvador", "recife", "fortaleza", "campinas", "goi√¢nia", "manaus",
        "brasil", "brazil", "brasileiro", "sudeste", "sul", "nordeste", "centro-oeste"
    ]
    
    # Palavras internacionais a evitar
    palavras_internacionais = [
        "united states", "new york", "london", "germany", "france", "canada", "australia",
        "usa", "uk", "europe", "middle east", "singapore", "dubai", "switzerland"
    ]
    
    # Verificar presen√ßa de palavras brasileiras
    tem_brasil = any(palavra in text_lower for palavra in palavras_brasil)
    tem_internacional = any(palavra in text_lower for palavra in palavras_internacionais)
    
    return tem_brasil and not tem_internacional

def is_vaga_executiva(text: str) -> bool:
    """Verifica se a vaga √© executiva/s√™nior"""
    text_lower = text.lower()
    executive_keywords = [
        "diretor", "gerente", "head", "l√≠der", "executivo", "supervisor", 
        "coordenador", "s√™nior", "pleno", "chief", "vp", "vice-presidente",
        "presidente", "s√≥cio", "partner", "c-level", "management", "gest√£o",
        "director", "manager", "lead", "chief"
    ]
    return any(kw in text_lower for kw in executive_keywords)

def normalize_title(title: str) -> str:
    """Normaliza√ß√£o avan√ßada para matching sem√¢ntico - PADR√ÉO EIGHTFOLD"""
    title_lower = title.lower()
    
    # Dicion√°rio de equival√™ncias brasileiras
    equivalencias = {
        "sr.": "s√™nior",
        "jr.": "j√∫nior",
        "supervisor": "coordenador",
        "tech lead": "l√≠der t√©cnico",
        "head of": "diretor de",
        "gerente de": "gerente",
        "diretor de": "diretor",
        "chief of": "diretor",
        "vp of": "diretor"
    }
    
    for orig, equiv in equivalencias.items():
        title_lower = title_lower.replace(orig, equiv)
    
    # Remover n√∫meros e s√≠mbolos
    title_clean = re.sub(r'[0-9\(\)\[\]\{\}\<\>\:\;\,\.\!\?\@\#\$\%\^\&\*\_\+\=\\\/]', '', title_lower)
    return title_clean.strip()

def detect_seniority(text: str, title: str = "") -> str:
    """Detec√ß√£o de senioridade com regras e fallback para an√°lise de texto - PADR√ÉO BEAMERY"""
    combined_text = (title + " " + text).lower()
    
    # Primeiro, verificar no t√≠tulo (mais importante)
    for rule in SENIORITY_RULES:
        if any(palavra in title.lower() for palavra in rule["palavras"]):
            return rule["nivel"]
    
    # Segundo, verificar na descri√ß√£o
    for rule in SENIORITY_RULES:
        if any(palavra in combined_text for palavra in rule["palavras"]):
            return rule["nivel"]
    
    # Fallback para anos de experi√™ncia
    if "5+ anos" in combined_text or "m√≠nimo de 5 anos" in combined_text or "5 anos" in combined_text:
        return "senior"
    elif "3+ anos" in combined_text or "m√≠nimo de 3 anos" in combined_text or "3 anos" in combined_text:
        return "pleno"
    
    return "pleno"  # Default seguro

def detect_area(text: str, title: str) -> str:
    """Detec√ß√£o de √°rea com regras especializadas"""
    combined_text = (title + " " + text).lower()
    
    for rule in AREA_RULES:
        if any(palavra in combined_text for palavra in rule["palavras"]):
            return rule["area"]
    
    return "operacoes"  # Default seguro

def extract_skills(text: str) -> list:
    """Extra√ß√£o avan√ßada de skills com categoriza√ß√£o - PADR√ÉO EIGHTFOLD"""
    text_lower = text.lower()
    skills_found = []
    
    # Banco de skills para Brasil (treinado com dados reais)
    skills_database = {
        "hard_skills": {
            "python": ["python", "django", "flask", "pandas", "numpy", "pytorch", "tensorflow"],
            "javascript": ["javascript", "react", "node.js", "typescript", "vue.js", "angular", "next.js"],
            "sql": ["sql", "postgresql", "mysql", "mariadb", "sql server", "bigquery", "snowflake"],
            "cloud": ["aws", "azure", "gcp", "cloud computing", "docker", "kubernetes", "terraform"],
            "data_science": ["machine learning", "deep learning", "ia", "intelig√™ncia artificial", "data science", "big data", "analytics"]
        },
        "soft_skills": {
            "lideranca": ["lideran√ßa", "gest√£o de equipe", "liderar", "team lead", "gest√£o de pessoas", "gest√£o de time"],
            "comunicacao": ["comunica√ß√£o", "apresenta√ß√£o", "negocia√ß√£o", "reuni√µes", "relacionamento", "stakeholders"],
            "resolucao_problemas": ["resolu√ß√£o de problemas", "an√°lise cr√≠tica", "pensamento l√≥gico", "solu√ß√£o de problemas", "an√°lise de dados"]
        },
        "tools": {
            "crm": ["salesforce", "hubspot", "pipedrive", "crm", "sap", "oracle"],
            "analytics": ["tableau", "power bi", "looker", "metabase", "google analytics", "sheets", "excel"]
        }
    }
    
    for category, skills in skills_database.items():
        for skill_name, keywords in skills.items():
            for keyword in keywords:
                if keyword in text_lower:
                    # Calcular n√≠vel de profici√™ncia com base no contexto
                    if "avan√ßado" in text_lower or "especialista" in text_lower or "expert" in text_lower:
                        proficiency = 5
                    elif "s√™nior" in text_lower or "dom√≠nio" in text_lower or "profici√™ncia" in text_lower:
                        proficiency = 4
                    elif "intermedi√°rio" in text_lower or "bom conhecimento" in text_lower or "conhecimento s√≥lido" in text_lower:
                        proficiency = 3
                    else:
                        proficiency = 2
                    
                    skills_found.append({
                        "name": skill_name.replace("_", " ").title(),
                        "normalized": skill_name,
                        "category": category,
                        "proficiency_level": proficiency,
                        "is_mandatory": "obrigat√≥rio" in text_lower or "requisito" in text_lower or "essencial" in text_lower,
                        "importance_weight": 90 if ("obrigat√≥rio" in text_lower or "requisito" in text_lower) else 70,
                        "raw_text": keyword
                    })
    
    # Remover duplicatas
    unique_skills = []
    seen = set()
    for skill in skills_found:
        if skill["normalized"] not in seen:
            seen.add(skill["normalized"])
            unique_skills.append(skill)
    
    return unique_skills

def generate_embeddings(text: str) -> list:
    """Gera embeddings para matching sem√¢ntico - PADR√ÉO EIGHTFOLD"""
    if not EMBEDDING_MODEL or not text:
        return []
    
    try:
        embedding = EMBEDDING_MODEL.encode(text, convert_to_tensor=False)
        return embedding.tolist()
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar embeddings: {e}")
        return []

def calculate_quality_score(vaga: dict) -> float:
    """Calcula score de qualidade baseado em crit√©rios do Lovable - PADR√ÉO BEAMERY"""
    score = 0.0
    
    # Descri√ß√£o completa (>200 caracteres)
    if len(vaga.get("description", "")) > 200:
        score += 0.2
    
    # Skills identificadas
    skills_required = vaga.get("skills_required", [])
    if skills_required and len(skills_required) > 0:
        score += 0.3
    
    # Sal√°rio divulgado
    if vaga.get("salary_disclosed") and vaga["salary_disclosed"]:
        score += 0.2
    
    # Localiza√ß√£o clara
    if vaga.get("city") and vaga.get("state"):
        score += 0.15
    
    # Modelo de trabalho definido
    work_model = vaga.get("work_model")
    if work_model and work_model in ["remote", "hybrid", "onsite"]:
        score += 0.15
    
    return min(score, 1.0)

def process_job_for_lovable(raw_vaga: dict) -> dict:
    """Processa vaga para o formato exato do Lovable - PADR√ÉO UNIC√ìRNIO"""
    # Gerar embeddings para matching sem√¢ntico
    description_embedding = generate_embeddings(raw_vaga["descricao_completa"])
    skills_text = " ".join([skill["name"] for skill in raw_vaga["skills_required"]])
    skills_embedding = generate_embeddings(skills_text)
    
    processed = {
        # 1. Identifica√ß√£o e Metadados
        "external_id": f"{raw_vaga['fonte']}_{hash(raw_vaga['link_candidatura'])}",
        "source": raw_vaga["fonte"],
        "source_url": raw_vaga["link_candidatura"],
        "scraped_at": datetime.utcnow().isoformat(),
        "posted_at": f"{raw_vaga['data_publicacao']}T00:00:00Z",
        "posted_days_ago": (datetime.now() - datetime.strptime(raw_vaga['data_publicacao'], "%Y-%m-%d")).days,
        "is_active": True,
        "is_verified": True,
        "ghost_job_risk_score": 0.1 if raw_vaga["fonte"] in ["LinkedIn", "Indeed", "Gupy"] else 0.3,
        
        # 2. Informa√ß√µes do Cargo
        "title": raw_vaga["cargo"],
        "title_normalized": normalize_title(raw_vaga["cargo"]),
        "seniority_level": detect_seniority(raw_vaga["descricao_completa"], raw_vaga["cargo"]),
        "area": detect_area(raw_vaga["descricao_completa"], raw_vaga["cargo"]),
        "sub_area": "",
        "sub_area_level_2": "",
        "sub_area_level_3": "",
        
        # 3. Informa√ß√µes da Empresa
        "company_name": raw_vaga["empresa"] if raw_vaga["empresa"] != "N√£o informado" else "Empresa n√£o informada",
        "company_name_normalized": normalize_title(raw_vaga["empresa"] if raw_vaga["empresa"] != "N√£o informado" else "empresa n√£o informada"),
        "is_headhunter": raw_vaga["fonte"] in ["Korn Ferry", "Spencer Stuart", "Egon Zehnder", "Heidrick & Struggles", "Russell Reynolds"],
        "is_tech_specialized": raw_vaga["fonte"] in ["LinkedIn", "Gupy", "Trampos.co", "Glassdoor", "Indeed"],
        
        # 4. Localiza√ß√£o e Modelo de Trabalho
        "city": "S√£o Paulo",  # Ser√° extra√≠do futuramente
        "state": "SP",       # Ser√° extra√≠do futuramente
        "country": "Brasil",
        # "region": extract_region(raw_vaga["localizacao"]),
        "work_model": raw_vaga["modalidade"].lower() if raw_vaga["modalidade"] != "N√£o informado" else "onsite",
        "is_remote_eligible": "remoto" in raw_vaga["modalidade"].lower() or "remote" in raw_vaga["modalidade"].lower(),
        "remote_countries": ["Brasil"],
        
        # 5. Remunera√ß√£o e Benef√≠cios
        "salary_min": None,
        "salary_max": None,
        "salary_median": None,
        "salary_disclosed": "sal√°rio" in raw_vaga["descricao_completa"].lower() or "remunera√ß√£o" in raw_vaga["descricao_completa"].lower(),
        "salary_type": "CLT",
        "currency": "BRL",
        "benefits": {},
        
        # 6. Skills e Requisitos
        "skills_required": raw_vaga["skills_required"],
        "experience_years_min": 5 if "5+ anos" in raw_vaga["descricao_completa"].lower() else 3 if "3+ anos" in raw_vaga["descricao_completa"].lower() else 2,
        "experience_years_max": None,
        
        # 7. Qualifica√ß√µes
        "education_required": [],
        "certifications_required": [],
        "languages_required": [],
        
        # 8. Conte√∫do e Descri√ß√£o
        "description": raw_vaga["descricao_completa"],
        "description_summary": raw_vaga["descricao_completa"][:200] + "..." if len(raw_vaga["descricao_completa"]) > 200 else raw_vaga["descricao_completa"],
        "responsibilities": [],
        "culture_keywords": ["resultados", "colabora√ß√£o", "inova√ß√£o", "excel√™ncia"],
        
        # 9. Embeddings e AI
        "embedding": json.dumps(description_embedding) if description_embedding else None,
        "skills_embedding": json.dumps(skills_embedding) if skills_embedding else None,
        
        # 10. M√©tricas e Analytics
        "view_count": 0,
        "application_count": 0,
        "competition_level": "alta" if raw_vaga["fonte"] == "LinkedIn" else ("m√©dia" if raw_vaga["fonte"] == "Indeed" else "baixa"),
        
        # Qualidade do dado
        "quality_score": 0.0  # Ser√° calculado abaixo
    }
    
    # Calcular qualidade
    processed["quality_score"] = calculate_quality_score(processed)
    
    return processed

def scrape_job_details(url: str, session: requests.Session) -> dict:
    """Scraping PROFUNDO com proxy rotativo - ESTRAT√âGIA SEEKOUT"""
    try:
        # Respeitar delays
        time.sleep(DELAY_ENTRE_REQUISICOES)
        
        # Tentativas com fallbacks
        for tentativa in range(3):
            try:
                res = session.get(url, timeout=15)
                if res.status_code == 200:
                    break
                logger.warning(f"Tentativa {tentativa+1} falhou com status {res.status_code}")
                time.sleep(5)
            except Exception as e:
                logger.warning(f"Tentativa {tentativa+1} falhou: {e}")
                time.sleep(5)
        else:
            logger.error(f"‚ùå Todas as tentativas falharam para {url}")
            return {
                "descricao_completa": f"Erro ao coletar detalhes da vaga em {url}",
                "salario": "N√£o informado",
                "modalidade": "N√£o informado"
            }
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # Extrair conte√∫do principal
        descricao = ""
        candidates = [
            "div.description", "div.job-description", "div.content", "article",
            "section.description", "div.job-details", "div.vacancy-description"
        ]
        
        for selector in candidates:
            elements = soup.select(selector)
            if elements:
                descricao = "\n".join([elem.get_text(strip=True) for elem in elements])
                if len(descricao) > 200:  # Conte√∫do significativo
                    break
        
        if not descricao:
            main = soup.select_one("main, #main, .main")
            descricao = main.get_text(strip=True) if main else "Descri√ß√£o n√£o encontrada"
        
        # Detectar informa√ß√µes extras
        salario = "N√£o informado"
        modalidade = "N√£o informado"
        
        if "sal√°rio" in descricao.lower() or "salario" in descricao.lower() or "R$" in descricao:
            salario = "Sal√°rio a combinar"
            if "R$" in descricao:
                # Tentar extrair valor aproximado
                match = re.search(r'R\$\s*([\d\.,]+)', descricao)
                if match:
                    salario = f"R$ {match.group(1)}"
        
        if "remoto" in descricao.lower() or "remote" in descricao.lower():
            modalidade = "remote"
        elif "presencial" in descricao.lower() or "on-site" in descricao.lower():
            modalidade = "onsite"
        elif "h√≠brido" in descricao.lower() or "hibrido" in descricao.lower() or "hybrid" in descricao.lower():
            modalidade = "hybrid"
        
        return {
            "descricao_completa": descricao[:2000] + "..." if len(descricao) > 2000 else descricao,
            "salario": salario,
            "modalidade": modalidade
        }
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao coletar detalhes da vaga {url}: {e}")
        return {
            "descricao_completa": f"Erro durante a coleta: {str(e)}",
            "salario": "N√£o informado",
            "modalidade": "N√£o informado"
        }

def scrape_google_jobs(query_base: str, days_back: int = 1) -> list:
    """Coleta inteligente com filtragem geogr√°fica e de qualidade - PADR√ÉO UNIC√ìRNIO"""
    all_jobs = []
    yesterday = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
    
    # Criar sess√£o com proxy
    session = get_proxy_session()
    
    logger.info(f"üåç INICIANDO COLETA COM FILTRAGEM GEOGR√ÅFICA BRASIL")
    logger.info(f"üîç Fontes configuradas: {len(SOURCES_BRASIL)} sites")
    logger.info(f"üìä Limite: {MAX_VAGAS_POR_FONTE} vagas por fonte, {MAX_VAGAS_TOTAIS} total")
    
    for source_query, location_filter in SOURCES_BRASIL:
        if len(all_jobs) >= MAX_VAGAS_TOTAIS:
            logger.info(f"üéØ Limite total de {MAX_VAGAS_TOTAIS} vagas atingido")
            break
        
        # Montar query com filtros geogr√°ficos e data
        search_query = f'{query_base} {source_query} {location_filter} after:{yesterday}'
        logger.info(f"üîç Buscando no Google (via SerpAPI): {search_query}")
        
        try:
            url = f"https://serpapi.com/search.json?q={urllib.parse.quote(search_query)}&hl=pt-BR&num={MAX_VAGAS_POR_FONTE*2}&api_key={SERPAPI_KEY}"
            res = requests.get(url, timeout=20)
            data = res.json()
            
            if "organic_results" not in data:
                logger.warning(f"‚ö†Ô∏è Nenhum resultado para: {search_query}")
                continue
            
            # Processar resultados com filtragem rigorosa
            vagas_fonte = 0
            for result in data["organic_results"]:
                if vagas_fonte >= MAX_VAGAS_POR_FONTE or len(all_jobs) >= MAX_VAGAS_TOTAIS:
                    break
                
                link = result.get("link", "")
                title = result.get("title", "Vaga sem t√≠tulo")
                
                # Filtros de qualidade imediatos
                if not link or len(link) < 10 or "google.com" in link or "url?" in link:
                    continue
                
                # Detectar fonte
                fonte = "Google"
                for site in [
                    "linkedin.com/jobs", "gupy.com.br", "vagas.com.br", "trampos.co",
                    "ciadetalentos.com.br", "glassdoor.com.br", "indeed.com", "roberthalf.com.br",
                    "michaelpage.com.br", "kornferry.com", "spencerstuart.com", "heidrick.com",
                    "russellreynolds.com", "pageexecutive.com", "talenses.com", "exec.com.br"
                ]:
                    if site in link:
                        fonte = site.split(".")[0].replace("com", "").replace("br", "").title()
                        break
                
                # Filtros de relev√¢ncia BRASIL + EXECUTIVA
                title_lower = title.lower()
                snippet = result.get("snippet", "")
                
                if not is_vaga_brasil(title + " " + link + " " + snippet):
                    logger.info(f"üåç Ignorando vaga internacional: {title[:50]}...")
                    continue
                
                if not is_vaga_executiva(title):
                    logger.info(f"üè¢ Ignorando vaga n√£o executiva: {title[:50]}...")
                    continue
                
                # Coletar detalhes com proxy
                details = scrape_job_details(link, session)
                
                # Extrair skills
                skills = extract_skills(details["descricao_completa"])
                
                # Montar registro completo
                job_record = {
                    "cargo": title.strip()[:100],
                    "empresa": "N√£o informado",
                    "salario": details["salario"][:50],
                    "modalidade": details["modalidade"][:30],
                    "data_publicacao": yesterday,
                    "localizacao": "Brasil",
                    "fonte": fonte,
                    "link_candidatura": link[:255],
                    "descricao_completa": details["descricao_completa"],
                    "skills_required": skills
                }
                
                all_jobs.append(job_record)
                vagas_fonte += 1
                logger.info(f"‚úÖ Coletada vaga RICA [{fonte}]: {title[:50]}... (Skills: {len(skills)})")
            
            logger.info(f"üìä Fonte '{fonte}': {vagas_fonte} vagas relevantes coletadas")
            time.sleep(3)  # Respeitar SerpAPI
        
        except Exception as e:
            logger.error(f"‚ùå Erro na busca do Google/SerpAPI para {source_query}: {e}")
            time.sleep(5)
    
    logger.info(f"‚úÖ COLETA FINALIZADA: {len(all_jobs)} vagas RICAS e RELEVANTES para o Brasil")
    return all_jobs

def save_to_supabase(vagas: list):
    """Salva vagas no Supabase com tratamento de erros - PADR√ÉO PRODU√á√ÉO"""
    logger.info("üíæ INICIANDO SALVAMENTO NO SUPABASE...")
    saved_count = 0
    errors_count = 0
    
    for vaga in vagas:
        try:
            # Processar para formato Lovable
            processed_vaga = process_job_for_lovable(vaga)
            
            # Salvar no Supabase
            supabase.table("vagas_lovable").insert(processed_vaga).execute()
            logger.info(f"‚úÖ Vaga salva: {processed_vaga['title'][:50]}... ({processed_vaga['source']})")
            saved_count += 1
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar vaga '{vaga.get('cargo', 'Sem t√≠tulo')[:30]}...': {e}")
            errors_count += 1
    
    logger.info(f"‚úÖ SALVAMENTO CONCLU√çDO: {saved_count} vagas salvas, {errors_count} erros")
    return saved_count

def run_scrapper():
    """Fun√ß√£o mestre de coleta inteligente - PADR√ÉO UNIC√ìRNIO"""
    logger.info("üöÄ INICIANDO COLETOR INTELIGENTE DE VAGAS (PADR√ÉO UNIC√ìRNIO)")
    logger.info("üéØ Foco: Vagas executivas no Brasil com alta qualidade de dados")
    
    # Coletar vagas com filtros rigorosos
    vagas = scrape_google_jobs(
        "diretor OR gerente OR head OR l√≠der OR executivo OR supervisor OR coordenador OR senior OR s√™nior OR c-level OR chief"
    )
    
    # Salvar no banco
    saved_count = save_to_supabase(vagas)
    
    # M√©tricas de qualidade
    logger.info("üìà M√âTRICAS FINAIS:")
    logger.info(f"   ‚Ä¢ Total de vagas coletadas: {len(vagas)}")
    logger.info(f"   ‚Ä¢ Vagas salvas com sucesso: {saved_count}")
    logger.info(f"   ‚Ä¢ Fontes utilizadas: {len(SOURCES_BRASIL)}")
    logger.info(f"   ‚Ä¢ Proxy rotativo: {'Ativo' if SCRAPERAPI_KEY else 'Inativo'}")
    logger.info(f"   ‚Ä¢ Embeddings: {'Ativo' if EMBEDDING_MODEL else 'Inativo'}")
    
    return saved_count

# Flask API
from flask import Flask

app = Flask(__name__)

@app.route("/health", methods=["GET"])
def health_check():
    """Endpoint de sa√∫de do servi√ßo"""
    return {
        "status": "online",
        "time": datetime.utcnow().isoformat(),
        "message": "‚úÖ Coletor Inteligente de Vagas est√° online! (Padr√£o Unic√≥rnio)",
        "config": {
            "max_vagas_por_fonte": MAX_VAGAS_POR_FONTE,
            "max_vagas_totais": MAX_VAGAS_TOTAIS,
            "fontes_configuradas": len(SOURCES_BRASIL),
            "proxy_ativo": bool(SCRAPERAPI_KEY),
            "embeddings_ativo": bool(EMBEDDING_MODEL)
        }
    }

if __name__ == "__main__":
    logger.info("üî• INICIANDO SERVIDOR - AGUARDANDO REQUISI√á√ïES")
    run_scrapper()  # Executar coleta imediatamente ao iniciar
    app.run(host="0.0.0.0", port=8000)
